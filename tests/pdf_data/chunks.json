[
  {
    "id": 0,
    "page_range": "Part 1/3",
    "text": "DARWING\u00d6DELMACHINE: OPEN-ENDEDEVOLUTION OFSELF-IMPROVINGAGENTS Jenny Zhang*,1,2 Shengran Hu*,1,2,3 Cong Lu1,2,3 Robert Lange\u2020,3 Jeff Clune\u2020,1,2,4 1University of British Columbia 2Vector Institute 3Sakana AI 4Canada CIFAR AI Chair {jennyzzt,srhu,conglu}@cs.ubc.ca,robert@sakana.ai,jeff.clune@ubc.ca ABSTRACT Most of today\u2019s AI systems are constrained by human-designed, fixed architectures and cannot autonomously and continuously improve themselves. The scientific method, on the other hand, is a cumulative and open-ended system, where each innovation builds upon previous artifacts, enabling future discoveries. There is growing hope that the current manual process of advancing AI could itself be automated. If done safely, such automation would accelerate AI development and allow us to reap its benefits much sooner. This prospect raises the question of how AI systems can endlessly improve themselves while getting better at solving relevant problems. Meta-learning can automate the discovery of novel algorithms, but is limited by first-order improvements and the human design of a suitable search space. The G\u00f6del machine (Schmidhuber, 2007) proposed a theoretical alternative: a self-improving AI that repeatedly modifies itself in a provably beneficial manner. Unfortunately, proving that most changes are net beneficial is impossible in practice. We introduce the Darwin G\u00f6del Machine (DGM), a novel self-improving system that iteratively modifies its own code (thereby also improving its ability to modify its own codebase) and empirically validates each change using coding benchmarks. Inspired by Darwinian evolution and open-endedness research, the DGM grows an archive of generated coding agents. It samples agents from this archive, which self-modify to create new, interesting versions of themselves. This open-ended exploration forms a growing tree of diverse, high-quality agents and allows the parallel exploration of many different paths through the search space. Empirically, the DGM automatically improves its coding capabilities (e.g., better code editing tools, long-context window management, peer-review mechanisms), increasing performance on SWE-bench from 20.0% to 50.0%, and on Polyglot from 14.2% to 30.7%. Furthermore, the DGM significantly outperforms baselines without self- improvement or open-ended exploration. All experiments were done with safety precautions (e.g., sandboxing, human oversight). Overall, the DGM represents a significant step toward self-improving AI, capable of gathering its own stepping stones along a path that unfolds into endless innovation. All code is open-sourced at https://github.com/jennyzzt/dgm. 1 INTRODUCTION Scientific progress is cumulative and open-ended, with each breakthrough standing on the shoulders of countless prior insights. In the same way, our most advanced AI systems are built upon a long lineage of innovations. For instance, transformers (Vaswani et al., 2017), the backbone of current large language models (LLMs) (Brown et al., 2020), did not emerge in isolation but were built upon years of past innovations, such as recurrent neural networks (Linnainmaa, 1970; Amari, 1972; Hopfield, 1982; Rumelhart et al., 1985) and attention mechanisms (Schmidhuber & Huber, 1990; Bahdanau et al., 2015; Kim et al., 2017; Parikh et al., 2016). However, most of today\u2019s AI systems remain bound by fixed, human-designed architectures that learn within predefined boundaries, without the capacity to autonomously rewrite their own source code to self-improve. As a result, each advancement in AI development still leans heavily on human interventions, tethering the pace of progress. This paper *co-authors \u2020 co-senior authors 1 arXiv:2505.22954v2 [cs.AI] 26 Sep 2025 investigates the intriguing possibility of safely automating the search for ever-better AI. One can imagine an AI system that, like scientific discovery itself, becomes an engine of its own advancement: building upon its past, recursively improving, and propelling itself toward more advanced capabilities. Archive New coding agent Coding agent Self - modify child Coding agent\u2019s own repo+Self-improveinstructionCode Diff:Feature to improve itselfCoding agent select Task repo(e.g., GitHub repo)+Task instruction(e.g., GitHub issue)Code Diff:Solve taskNew coding agent Evaluate on benchmarkadd parent Figure 1:Darwin G\u00f6del Machine.The DGM iteratively builds a growing archive of agents by interleaving self-modification with downstream task evaluation. Agents in the archive are selected for self-modification through open-ended exploration. Schmidhuber (2007) presented a class of mathematically rigorous, self-referential, self-improving problem solvers. It relies on formal proofs to justify code rewrites, ensuring that any self-modification is provably beneficial. However, in practice and without restrictive assumptions about the system, it is impossible to formally prove whether a modification to an AI system will be beneficial. For example, while it may seem that an LLM-based coding agent would benefit from access to more tools (e.g., code search, test runners), the actual impact depends heavily on the model\u2019s training and task context (e.g., a testing tool that is optimized for one setup may confuse the agent when working with others). Instead of requiring formal proofs, we empirically validate self-modifications against a benchmark, allowing the system to improve and explore based on observed results. This approach mirrors biological evolution, where mutations and adaptations are not verified in advance but are produced, trialed, and then selected via natural selection. We also take inspiration from Darwinian evolution (Darwin, 2023) and investigate the effectiveness of maintaining a library of previously discovered agents to serve as stepping stones for future generations. We propose theDarwin G\u00f6del Machine (DGM), a self-referential, self-improving system that writes and modifies its own code to become a better coding agent. Each self-modification requires the DGM to edit its own codebase. We use Python, which is Turing-complete, giving the DGM the potential to build any computable machine. Our framework envisions agents that can rewrite their own training scripts (including training a new foundation model (FM)). However, we do not show that in this paper, as training FMs is computationally intensive and would introduce substantial additional complexity, which we leave as future work. Instead, this paper focuses on improving the design of coding agents with frozen pretrained FMs (e.g., tool use, workflows). The DGM alternates between self-modification and evaluation phases. During the self-modification phase, selected coding agents from the archive generate modified versions of themselves. During the evaluation phase, each modified agent is tested on a coding benchmark, estimating the agent\u2019s coding capabilities, and then added to the archive. By improving its own capabilities through this loop, the DGM becomes better at both solving coding tasks and making future self-improvements. A key assumption is that an increase in performance on coding benchmarks indicates better coding capabilities, and hence better ability to self-modify and self-improve. Furthermore, the DGM maintains an archive of generated coding agents, initialized with only one agent, and continuously accumulates all generated variants over time. To support continual self-improvement, the DGM draws inspiration from open- endedness research (Wang et al., 2019; Fernando et al., 2024; Faldor et al., 2025), accumulating diverse stepping stones (i.e., interesting yet suboptimal solutions or features that may enable future breakthroughs). This open-ended exploration encourages the discovery of novel and potentially useful self-modifications beyond immediate performance gains. We present results on two coding benchmarks: SWE-bench (Jimenez et al., 2024) and Polyglot (Paul Gauthier, 2024). The DGM automatically improves itsel",
    "char_count": 73279
  },
  {
    "id": 1,
    "page_range": "Part 2/3",
    "text": "s Sutton. Lambdabeam: Neural pro- gram search with higher-order functions and lambdas.Advances in Neural Information Processing Systems, 36:51327\u201351346, 2023. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning.Advances in Neural Information Processing Systems, 36:8634\u20138652, 2023. David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search.nature, 529(7587):484\u2013489, 2016. 18 David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. Mastering chess and shogi by self-play with a general reinforcement learning algorithm.arXiv preprint arXiv:1712.01815, 2017. Avi Singh, John D Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Xavier Garcia, Peter J Liu, James Harrison, Jaehoon Lee, Kelvin Xu, et al. Beyond human data: Scaling self-training for problem-solving with language models.arXiv preprint arXiv:2312.06585, 2023. Joar Skalse, Nikolaus Howe, Dmitrii Krasheninnikov, and David Krueger. Defining and characterizing reward gaming.Advances in Neural Information Processing Systems, 35:9460\u20139471, 2022. Kenneth O Stanley and Joel Lehman.Why greatness cannot be planned: The myth of the objective. Springer, 2015. Kenneth O Stanley, Joel Lehman, and Lisa Soros. Open-endedness: The last grand challenge you\u2019ve never heard of.While open-endedness could be a force for discovering intelligence, it could also be a component of AI itself, 2017. Marilyn Strathern. \u2018Improving ratings\u2019: audit in the British University system.European review, 5 (3):305\u2013321, 1997. Jinwei Su, Yinghui Xia, Ronghua Shi, Jianhui Wang, Jianuo Huang, Yijin Wang, Tianyu Shi, Yang Jingsong, and Lewei He. Debflow: Automating agent creation via agent debate.arXiv preprint arXiv:2503.23781, 2025. Shyam Sudhakaran, Miguel Gonz\u00e1lez-Duque, Matthias Freiberger, Claire Glanois, Elias Najarro, and Sebastian Risi. Mariogpt: Open-ended text2level generation through large language models. Advances in Neural Information Processing Systems, 36:54213\u201354227, 2023. OpenAI Team, Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need.Advances in neural information processing systems, 30, 2017. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. V oyager: An open-ended embodied agent with large language models.arXiv preprint arXiv:2305.16291, 2023a. Ren-Jian Wang, Ke Xue, Yutong Wang, Peng Yang, Haobo Fu, Qiang Fu, and Chao Qian. Diversity from human feedback.arXiv preprint arXiv:2310.06648, 2023b. Rui Wang, Joel Lehman, Jeff Clune, and Kenneth O Stanley. Paired open-ended trailblazer (poet): Endlessly generating increasingly complex and diverse learning environments and their solutions. arXiv preprint arXiv:1901.01753, 2019. Xingyao Wang, Boxuan Li, Yufan Song, Frank F Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, et al. Openhands: An open platform for ai software develop- ers as generalist agents. InThe Thirteenth International Conference on Learning Representations, 2024. Yufei Wang, Zhou Xian, Feng Chen, Tsun-Hsuan Wang, Yian Wang, Katerina Fragkiadaki, Zackory Erickson, David Held, and Chuang Gan. Robogen: Towards unleashing infinite data for automated robot learning via generative simulation.arXiv preprint arXiv:2311.01455, 2023c. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models.Advances in neural information processing systems, 35:24824\u201324837, 2022. Marco Wiering and J\u00fcrgen Schmidhuber. Hq-learning.Adaptive behavior, 6(2):219\u2013246, 1997. 19 S Wright. The roles of mutation, inbreeding, crossbreeding and selection in evolution, proceedings of the sixth international congress of genetics. proc sixth int congr genet [internet].New York356366, 1932. Chunqiu Steven Xia, Yinlin Deng, Soren Dunn, and Lingming Zhang. Agentless: Demystifying llm-based software engineering agents.arXiv preprint arXiv:2407.01489, 2024. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. InInternational Conference on Learning Representations (ICLR), 2023. Rui Ye, Shuo Tang, Rui Ge, Yaxin Du, Zhenfei Yin, Siheng Chen, and Jing Shao. Mas-gpt: Training llms to build llm-based multi-agent systems.arXiv preprint arXiv:2503.03686, 2025. Xunjian Yin, Xinyi Wang, Liangming Pan, Xiaojun Wan, and William Yang Wang. G \\\" odel Agent: A Self-Referential Agent Framework for Recursive Self-Improvement.arXiv preprint arXiv:2410.04444, 2024. Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Dongsheng Li, and Deqing Yang. Evoagent: Towards automatic multi-agent generation via evolutionary algorithms.arXiv preprint arXiv:2406.14228, 2024. Eliezer Yudkowsky et al. Artificial Intelligence as a positive and negative factor in global risk.Global catastrophic risks, 1(303):184, 2008. Mert Yuksekgonul, Federico Bianchi, Joseph Boen, Sheng Liu, Zhi Huang, Carlos Guestrin, and James Zou. Textgrad: Automatic\" differentiation\" via text.arXiv preprint arXiv:2406.07496, 2024. Eric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, and Noah D Goodman. Quiet-star: Language models can teach themselves to think before speaking.arXiv preprint arXiv:2403.09629, 2024a. Eric Zelikman, Eliana Lorch, Lester Mackey, and Adam Tauman Kalai. Self-taught optimizer (stop): Recursively self-improving code generation. InFirst Conference on Language Modeling, 2024b. Dan Zhang, Sining Zhoubian, Ziniu Hu, Yisong Yue, Yuxiao Dong, and Jie Tang. Rest-mcts*: Llm self-training via process reward guided tree search.Advances in Neural Information Processing Systems, 37:64735\u201364772, 2024a. Guibin Zhang, Luyang Niu, Junfeng Fang, Kun Wang, Lei Bai, and Xiang Wang. Multi-agent architecture search via agentic supernet.arXiv preprint arXiv:2502.04180, 2025a. Jenny Zhang, Joel Lehman, Kenneth Stanley, and Jeff Clune. OMNI: Open-endedness via Models of human Notions of Interestingness. InThe Twelfth International Conference on Learning Representations, 2024b. URLhttps://openreview.net/forum?id=AgM3MzT99c. Jiayi Zhang, Jinyu Xiang, Zhaoyang Yu, Fengwei Teng, Xionghui Chen, Jiaqi Chen, Mingchen Zhuge, Xin Cheng, Sirui Hong, Jinlin Wang, et al. Aflow: Automating agentic workflow generation.arXiv preprint arXiv:2410.10762, 2024c. Yuanshuo Zhang, Yuchen Hou, Bohan Tang, Shuo Chen, Muhan Zhang, Xiaowen Dong, and Siheng Chen. Gnns as predictors of agentic workflow performances.arXiv preprint arXiv:2503.11301, 2025b. Yuntong Zhang, Haifeng Ruan, Zhiyu Fan, and Abhik Roychoudhury. Autocoderover: Autonomous program improvement. InProceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis, pp. 1592\u20131604, 2024d. Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin Wang. Memorybank: Enhancing large language models with long-term memory. InProceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 19724\u201319731, 2024. 20 Andy Zhou, Kevin Wu, Francesco Pinto, Zhaorun Chen, Yi Zeng, Yu Yang, Shuang Yang, Sanmi Koyejo, James Zou, and Bo Li. AutoRedTeamer: Autonomous Red Teaming with Lifelong Attack Integration.arXiv preprint arXiv:2503.15754, 2025. Wangchunshu Zhou, Yixin Ou, Shengwei Ding, Long Li, Jialong Wu, Tiannan Wang, Jiamin Chen, Shuai Wang, Xiaohu",
    "char_count": 73129
  },
  {
    "id": 2,
    "page_range": "Part 3/3",
    "text": "problem. Your solution must include ,\u2192changes to the main source code files, not just test files.\"\"\" # Run the agent new_msg_history = chat_with_agent(instruction, model=self.code_model, ,\u2192msg_history=[], logging=safe_log) @@ -228,16 +238,45 @@ Please ensure your solution includes changes to the main source code ,\u2192files, not if is_valid: safe_log(f\"Valid patch generated: {reason}\") - break + # Run regression tests for this candidate + test_report = self.run_regression_tests(regression_tests_summary) + test_score = get_report_score(test_report) + safe_log(f\"Test score: {test_score}\") + + valid_patches.append(patch) + valid_reports.append(test_report) + + if len(valid_patches) >= self.num_candidates: + break else: safe_log(f\"Invalid patch: {reason}\") - if retry_count < self.max_retries - 1: - safe_log(\"Retrying with a new attempt...\") 46 - else: - safe_log(\"Maximum retries reached. Unable to generate a valid patch.\") retry_count += 1 + if not valid_patches: + safe_log(\"Failed to generate any valid patches.\") + return + + # Use score_tie_breaker to select the best patch + safe_log(f\"\\n=== Selecting Best Solution from {len(valid_patches)} Candidates ===\") + best_index = score_tie_breaker( + self.problem_statement, + valid_patches, + valid_reports, + logging=safe_log + ) + + # Reset to base and apply the best patch + reset_to_commit(self.git_tempdir, self.base_commit) + best_patch = valid_patches[best_index] + safe_log(f\"\\n=== Applying Best Solution (Candidate {best_index + 1}) ===\") + apply_patch(self.git_tempdir, best_patch) + + # Final validation of the selected patch + final_test_report = self.run_regression_tests(regression_tests_summary) + final_score = get_report_score(final_test_report) + safe_log(f\"Final solution test score: {final_score}\") + def main(): parser = argparse.ArgumentParser(description='Process repository with an agentic ,\u2192system.') parser.add_argument('--problem_statement', required=True, help='The problem statement ,\u2192to process') @@ -249,6 +288,7 @@ def main(): parser.add_argument('--self_improve', default=False, action='store_true', ,\u2192help='Whether to self-improve the repository or solving swe') parser.add_argument('--instance_id', default=None, help='Instance ID for SWE issue') parser.add_argument('--max_retries', type=int, default=3, help='Maximum number of ,\u2192patch generation attempts') + parser.add_argument('--num_candidates', type=int, default=3, help='Number of candidate ,\u2192solutions to generate') args = parser.parse_args() # Process the repository @@ -261,6 +301,7 @@ def main(): self_improve=args.self_improve, instance_id=args.instance_id, max_retries=args.max_retries, + num_candidates=args.num_candidates, ) # Run the agentic system to try to solve the problem diff--git a/coding_agent.py b/coding_agent.py index3f1bc1d..588938d 100644 --- a/coding_agent.py +++ b/coding_agent.py @@ -193,42 +193,59 @@ Your task is to run the regression tests in the {self.git_tempdir} ,\u2192directory to def forward(self): \"\"\" The forward function for the AgenticSystem that generates and evaluates multiple ,\u2192candidate patches. + This version maintains history of prior valid patches and test results, only using the ,\u2192tie-breaker + when necessary. \"\"\" - base_instruction = f\"\"\"I have uploaded a Python code repository in the directory ,\u2192{self.git_tempdir}. Help solve the following problem. - -<problem_description> -{self.problem_statement} -</problem_description> - -<test_description> -{self.test_description} -</test_description> - 47 -Your task is to make changes to the files in the {self.git_tempdir} directory to address ,\u2192the <problem_description>. I have already taken care of the required dependencies. -\"\"\" - - # Get regression tests summary once at the start regression_tests_summary = self.get_regression_tests() - # Lists to store candidates + # Lists to store all valid patches and their information valid_patches = [] valid_reports = [] + valid_scores = [] + best_score = 0 + best_patches_indices = [] # Indices of patches that share the best score retry_count = 0 while retry_count < self.max_retries and len(valid_patches) < self.num_candidates: safe_log(f\"\\n=== Attempt {retry_count + 1} of {self.max_retries} ===\") safe_log(f\"Valid solutions so far: {len(valid_patches)} of {self.num_candidates} ,\u2192desired\") + safe_log(f\"Current best test score: {best_score}\") # Reset to base commit before each attempt if retry_count > 0: reset_to_commit(self.git_tempdir, self.base_commit) - # Add retry context to instruction if this is a retry attempt - instruction = base_instruction - if retry_count > 0: - instruction += f\"\"\"\\nNOTE: Previous attempt(s) did not produce enough valid solutions. + # Construct instruction with previous best solutions if available + instruction = f\"\"\"I have uploaded a Python code repository in the directory ,\u2192{self.git_tempdir}. Help solve the following problem. + +<problem_description> +{self.problem_statement} +</problem_description> + +<test_description> +{self.test_description} +</test_description>\"\"\" + + # Add previous solutions context if available + if valid_patches and retry_count > 0: + previous_solutions = [] + for i, (patch, report, score) in enumerate(zip(valid_patches, valid_reports, ,\u2192valid_scores)): + previous_solutions.append(f\"\"\" +Previous Solution {i+1}: +<code_changes> +{patch} +</code_changes> +Test Score: {score} +Test Report: {report} +\"\"\") + instruction += \"\\n\\nPrevious solution attempts:\\n\" + \"\\n\".join(previous_solutions) + instruction += \"\\nPlease provide a new solution that addresses any limitations in the ,\u2192previous attempts or explores a different approach.\" + elif retry_count > 0: + instruction += \"\"\"\\nNOTE: Previous attempt(s) did not produce enough valid solutions. Please provide a different approach to solve the problem. Your solution must include ,\u2192changes to the main source code files, not just test files.\"\"\" + instruction += f\"\\n\\nYour task is to make changes to the files in the {self.git_tempdir} ,\u2192directory to address the <problem_description>. I have already taken care of the ,\u2192required dependencies.\" + # Run the agent new_msg_history = chat_with_agent(instruction, model=self.code_model, ,\u2192msg_history=[], logging=safe_log) @@ -245,6 +262,14 @@ Please provide a different approach to solve the problem. Your ,\u2192solution must inc valid_patches.append(patch) valid_reports.append(test_report) + valid_scores.append(test_score) + + # Update best score and indices + if test_score > best_score: + best_score = test_score 48 + best_patches_indices = [len(valid_patches) - 1] + elif test_score == best_score: + best_patches_indices.append(len(valid_patches) - 1) if len(valid_patches) >= self.num_candidates: break @@ -257,25 +282,30 @@ Please provide a different approach to solve the problem. Your ,\u2192solution must inc safe_log(\"Failed to generate any valid patches.\") return - # Use score_tie_breaker to select the best patch + # Only use tie-breaker if we have multiple patches with the best score safe_log(f\"\\n=== Selecting Best Solution from {len(valid_patches)} Candidates ===\") - best_index = score_tie_breaker( - self.problem_statement, - valid_patches, - valid_reports, - logging=safe_log - ) + if len(best_patches_indices) > 1: + safe_log(f\"Multiple solutions ({len(best_patches_indices)}) tied for best score ,\u2192{best_score}. Using tie-breaker.\") + best_index = score_tie_breaker( + self.problem_statement, + [valid_patches[i] for i in best_patches_indices], + [valid_reports[i] for i in best_patches_indices], + logging=safe_log + ) + best_index = best_patches_indices[best_index] + else: + best_index = best_patches_indices[0] # Reset to base and apply the best patch reset_to_commit(self.git_tempdir, self.base_commit) best_patch = valid_patches[best_index] - safe_log(f\"\\n=== Applying Best Solution (Candidate {best_index + 1}) ===\") + safe_log(f\"\\n=== Applying Best Solution (Candidate {best_index + 1}) with score ,\u2192{valid_scores[best_index]} ===\") apply_patch(self.git_tempdir, best_patch) # Final validation of the selected pat",
    "char_count": 73123
  }
]