# EmergentDB Benchmarks

Fair, reproducible benchmarks comparing EmergentDB against other vector databases.

## Philosophy: Apples-to-Apples Comparisons

Benchmarking vector databases is tricky. Unfair comparisons are easy to make:
- Comparing optimized configs to defaults
- Adding HTTP overhead to one but not the other
- Using synthetic vectors vs real embeddings
- Ignoring retrieval quality for raw speed

This document establishes our benchmarking methodology to ensure **fair, reproducible, and meaningful** comparisons.

---

## Methodology

### 1. Same Embeddings
All databases receive **identical embeddings** from the same source. We use Gemini `gemini-embedding-001` (768D) for real-world relevance.

### 2. Same Document
Tests use the same document chunks - no cherry-picking data that favors one implementation.

### 3. Same Queries
Identical queries across all configurations, generated by LLM to avoid bias.

### 4. No Network Overhead
- EmergentDB: Native Rust binary (in-process)
- Competitors: In-process library calls (no HTTP)

### 5. Fair Configuration Pairs
When comparing index types, we match parameters:
- HNSW vs HNSW with same M, ef_construction, ef_search
- Flat vs Flat (brute force)

### 6. Quality Validation
Speed without accuracy is meaningless. We use LLM-as-judge to score retrieval quality (0-10) and report both metrics.

---

## RAG Benchmark Results

**Test Setup:**
| Parameter | Value |
|-----------|-------|
| Document | arXiv paper 2005.11401 (RAG paper) |
| Chunks | 173 (500 chars, 100 overlap) |
| Embeddings | Gemini embedding-001 (768D) |
| Queries | 5 LLM-generated factual questions |
| k | 5 results per query |
| Hardware | Apple Silicon (ARM NEON SIMD) |

### Results

| Configuration | Index | Latency | Quality | Build Time |
|---------------|-------|---------|---------|------------|
| EmergentDB (Evolved) | Flat | 43.6μs | 8.2/10 | 617ms |
| EmergentDB (SIMD Flat) | Flat | 44.5μs | 8.0/10 | 0ms |
| EmergentDB (HNSW) | HNSW | 51.4μs | 9.0/10 | 46ms |
| EmergentDB (HNSW High-Recall) | HNSW | 88.6μs | 9.6/10 | 125ms |
| ChromaDB (Max Recall) | HNSW | 356.3μs | 9.0/10 | 38ms |
| ChromaDB (Default) | HNSW | 368.3μs | 9.0/10 | 47ms |
| ChromaDB (Tuned) | HNSW | 366.1μs | 9.4/10 | 37ms |

### Speedup Analysis

| Comparison | Speedup | Type |
|------------|---------|------|
| EmergentDB HNSW vs ChromaDB HNSW (same params) | **4.0x** | Implementation |
| EmergentDB SIMD Flat vs ChromaDB Default | **8.3x** | Out-of-box |
| EmergentDB Evolved vs ChromaDB Default | **8.4x** | Auto-tuned |

### Key Insight

The **4.0x speedup with identical HNSW parameters** demonstrates pure implementation advantage:
- SIMD-optimized distance calculations (ARM NEON / AVX2)
- Cache-friendly memory layout
- Rust vs Python runtime

The additional speedup from MAP-Elites comes from auto-selecting the optimal index type for the data characteristics.

---

## Index Types Explained

| Index | Complexity | Best For |
|-------|-----------|----------|
| **Flat** | O(n) | Small datasets (<10K), exact results |
| **HNSW** | O(log n) | Medium-large datasets, high recall |
| **Evolved** | Adaptive | Unknown workloads, auto-optimization |

### When Flat Beats HNSW

For small datasets (< 1K vectors), SIMD-optimized brute force is often faster than graph traversal overhead. MAP-Elites automatically discovers this.

---

## Reproduce

```bash
# 1. Clone and setup
git clone https://github.com/emergentdb/emergentdb
cd emergentdb

# 2. Build Rust benchmark
cargo build --release --example comprehensive_rag_benchmark

# 3. Setup Python environment
cd tests
uv venv --python 3.12 .venv
source .venv/bin/activate
uv pip install chromadb google-genai python-dotenv PyMuPDF numpy

# 4. Set Gemini API key
echo "GEMINI_API_KEY=your_key_here" > .env

# 5. Run benchmark
python comprehensive_rag_benchmark.py
```

---

## Contributing Benchmarks

We welcome benchmark contributions! Please ensure:

1. **Fair comparisons** - Match configurations where possible
2. **Real data** - Prefer real embeddings over random vectors
3. **Quality metrics** - Report retrieval quality, not just speed
4. **Reproducible** - Include all setup steps and dependencies
5. **Hardware noted** - Results vary by CPU architecture

---

## Benchmark Files

| File | Description |
|------|-------------|
| `examples/comprehensive_rag_benchmark.rs` | Native Rust benchmark (all index types) |
| `examples/rag_benchmark.rs` | Simple Rust RAG benchmark |
| `tests/comprehensive_rag_benchmark.py` | Full comparison with ChromaDB |

---

*Last updated: January 18, 2026*
